{
    "huggingface": {
        "mistral7b": {
            "params": {
                "temperature": 0.6,
                "top_p": 0.4,
                "max_tokens": 256,
                "seed": 42,
                "stream": true            
            },
            "inference_api": "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.3"
        },
        "mixtral":{
            "params": {
                "temperature": 0.6,
                "top_p": 0.4,
                "max_tokens": 512,
                "seed": 42,
                "stream": true            
            },
            "inference_api": "https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1"
        },
        "llama3":{
            "params": {
                "temperature": 0.6,
                "top_p": 0.4,
                "max_tokens": 512,
                "seed": 42,
                "stream": true            
            },
            "inference_api": "https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct"
        },
        "stabilityai/stablelm-zephyr-3b":{
            "params": {
                "temperature": 0.6,
                "top_p": 0.4,
                "max_tokens": 512,
                "seed": 42,
                "stream": true            
            },
            "inference_api": "https://api-inference.huggingface.co/models/stabilityai/stablelm-zephyr-3b"
        }
        
    },
    "groq": {
        "mixtral":{
            "params": {
                "model": "mixtral-8x7b-32768",
                "temperature": 0.2,
                "top_p": 0.4,
                "max_tokens": 1024,
                "stream": true            },
            "inference_api": "https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1"
        },
        "gemma7b":{
            "params": {
                "model": "gemma-7b-it",
                "temperature": 0.2,
                "top_p": 0.4,
                "max_tokens": 1024,
                "stream": true            
            },
            "inference_api": "https://huggingface.co/google/gemma-7b-it"
        }
    },
    "ngc": {
        "mixtral":{
            "params": {
                "model": "mistralai/mixtral-8x7b-instruct-v0.1",
                "temperature": 0.5,
                "top_p": 1,
                "max_tokens": 1024,
                "stream": true            },
            "inference_api": "https://integrate.api.nvidia.com/v1"
        },
        "gemma7b":{
            "params": {
                "model": "gemma-7b-it",
                "temperature": 0.2,
                "top_p": 0.4,
                "max_tokens": 1024,
                "stream": true            
            },
            "inference_api": "https://huggingface.co/google/gemma-7b-it"
        }
    },
    "anthropic": {
        "haiku":{
            "params": {
                "model": "claude-3-haiku-20240307",
                "temperature": 0.5,
                "top_p": 0.4,
                "max_tokens": 1024       },
            "inference_api": "https://api.anthropic.com/v1/messages"
        },
        "sonnet":{
            "params": {
                "model": "claude-3-sonnet-20240229",
                "temperature": 0.5,
                "top_p": 0.4,
                "max_tokens": 1024
            },
            "inference_api": "https://api.anthropic.com/v1/messages"
        },
        "opus":{
            "params": {
                "model": "claude-3-opus-20240229",
                "temperature": 0.5,
                "top_p": 0.4,
                "max_tokens": 1024
            },
            "inference_api": "https://api.anthropic.com/v1/messages"
        }
    },
    "cohere": {
        "command-r":{
            "params": {
                "model": "command-r",
                "temperature": 0.3,
                "prompt_truncation": "AUTO"
            },
            "inference_api": "https://api.cohere.ai/v1/chat"
        }
    },
    "openrouter": {
        "mistral7b":{
            "params": {
                "model": "mistralai/mistral-7b-instruct:free",
                "temperature": 0.5,
                "top_p": 1,
                "max_tokens": 1024
            },
            "inference_api": "https://openrouter.ai/api/v1"
        }
    }
}
