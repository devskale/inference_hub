{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5cOHvMEI_vG",
        "outputId": "a25b4c72-9369-4176-d020-591b6e35834e"
      },
      "outputs": [],
      "source": [
        "# !pip -q install gradio langchain-openai langchain-community langchain langchain-google-genai langchain-anthropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baFmdZ4pcQOG",
        "outputId": "77739a9f-2ef2-469f-b0b5-a47c93a73acb"
      },
      "outputs": [],
      "source": [
        "# !pip show gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WYH28kmuJTNq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIzaSyAgrLrWGPs9n5ijbPu52VPbhWI7ykL-stI\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getparams import load_api_credentials\n",
        "\n",
        "#from google.colab import userdata\n",
        "gemini_api_key = load_api_credentials('gemini')\n",
        "print(gemini_api_key)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OGKoUYrXJpwW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/johannwaldherr/code/Inference/inference_hub/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i2fvcoLxWdvS"
      },
      "outputs": [],
      "source": [
        "system_message = \"You are a helpful assistant who acts like a pirate.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "GQth439KJGG_",
        "outputId": "e8e72ac2-b1bc-4035-97ac-22413f788787"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/johannwaldherr/code/Inference/inference_hub/.venv/lib/python3.12/site-packages/gradio/components/chatbot.py:228: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://006dbfa06f9d814f5f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://006dbfa06f9d814f5f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: hej, how are ya. History: []\n",
            "\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://006dbfa06f9d814f5f.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Initialize chat model\n",
        "# llm = ChatOpenAI(temperature=0.7, model='gpt-4o-mini', streaming=True)\n",
        "\n",
        "# Initialize Gemini AI Studio chat model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-002\", streaming=True, api_key=load_api_credentials('gemini'))\n",
        "\n",
        "# Initialize Gemini AI Studio chat model\n",
        "#llm = ChatAnthropic(model='claude-3-haiku-20240307', streaming=True)\n",
        "\n",
        "\n",
        "\n",
        "def stream_response(message, history):\n",
        "    print(f\"Input: {message}. History: {history}\\n\")\n",
        "\n",
        "    history_langchain_format = []\n",
        "    history_langchain_format.append(SystemMessage(content=system_message))\n",
        "\n",
        "    for human, ai in history:\n",
        "        history_langchain_format.append(HumanMessage(content=human))\n",
        "        history_langchain_format.append(AIMessage(content=ai))\n",
        "\n",
        "    if message is not None:\n",
        "        history_langchain_format.append(HumanMessage(content=message))\n",
        "        partial_message = \"\"\n",
        "        for response in llm.stream(history_langchain_format):\n",
        "            partial_message += response.content\n",
        "            yield partial_message\n",
        "\n",
        "\n",
        "demo_interface = gr.ChatInterface(\n",
        "\n",
        "    stream_response,\n",
        "    textbox=gr.Textbox(placeholder=\"Send to the LLM...\",\n",
        "                       container=False,\n",
        "                       autoscroll=True,\n",
        "                       scale=7),\n",
        ")\n",
        "\n",
        "demo_interface.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmvZTgrNK_XZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
